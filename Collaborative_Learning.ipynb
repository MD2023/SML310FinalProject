{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HY-4lqin2s6f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import argparse\n",
    "class RoadSignClassifier:\n",
    "    def createCNN(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(8, (5, 5), input_shape=inputShape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "data_path_training = r\"D:\\TrafficSign\\Collaborative Learning\\Collaborative Learning\"\n",
    "def load_training_data(dataset):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        # print(img_path)\n",
    "        # print(img_class)\n",
    "        # print(i)\n",
    "        # print(row)\n",
    "        image = os.path.join(data_path_training, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        image_rs = cv2.resize(image, (32, 32), 3)\n",
    "        R, G, B = cv2.split(image_rs)\n",
    "        img_r = cv2.equalizeHist(R)\n",
    "        img_g = cv2.equalizeHist(G)\n",
    "        img_b = cv2.equalizeHist(B)\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(img_class)\n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    \n",
    "    return (X, y)\n",
    "\n",
    "data_path_testing= r\"D:\\TrafficSign\\Collaborative Learning\"      \n",
    "def load_testing_data(dataset):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "    # print(rows)\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        # print(img_path)\n",
    "        # print(img_class)\n",
    "        image = os.path.join(data_path_testing, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        # print(image)\n",
    "        image_rs = cv2.resize(image, (32, 32), 3)\n",
    "        R, G, B = cv2.split(image_rs)\n",
    "        img_r = cv2.equalizeHist(R)\n",
    "        img_g = cv2.equalizeHist(G)\n",
    "        img_b = cv2.equalizeHist(B)\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(int(img_class))\n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    \n",
    "    return (X, y)\n",
    "\n",
    "class AdditionalValidationSets(Callback):\n",
    "    def __init__(self, validation_sets, verbose=1, batch_size=None):\n",
    "        \"\"\"\n",
    "        :param validation_sets:\n",
    "        a list of 3-tuples (validation_data, validation_targets, validation_set_name)\n",
    "        or 4-tuples (validation_data, validation_targets, sample_weights, validation_set_name)\n",
    "        :param verbose:\n",
    "        verbosity mode, 1 or 0\n",
    "        :param batch_size:\n",
    "        batch size to be used when evaluating on the additional datasets\n",
    "        \"\"\"\n",
    "        super(AdditionalValidationSets, self).__init__()\n",
    "        self.validation_sets = validation_sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) not in [3, 4]:\n",
    "                raise ValueError()\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch = []\n",
    "        self.history = {}\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epoch.append(epoch)\n",
    "\n",
    "        # record the same values as History() as well\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "\n",
    "        # evaluate on the additional validation sets\n",
    "        for validation_set in self.validation_sets:\n",
    "            if len(validation_set) == 3:\n",
    "                validation_data, validation_targets, validation_set_name = validation_set\n",
    "                sample_weights = None\n",
    "            elif len(validation_set) == 4:\n",
    "                validation_data, validation_targets, sample_weights, validation_set_name = validation_set\n",
    "            else:\n",
    "                raise ValueError()\n",
    "\n",
    "            results = self.model.evaluate(x=validation_data,\n",
    "                                          y=validation_targets,\n",
    "                                          verbose=self.verbose,\n",
    "                                          sample_weight=sample_weights,\n",
    "                                          batch_size=self.batch_size)\n",
    "\n",
    "            for metric, result in zip(self.model.metrics_names,results):\n",
    "                valuename = validation_set_name + '_' + metric\n",
    "                self.history.setdefault(valuename, []).append(result)\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-m\", \"--model\", default=\"output/collaborativetrafficsignnet.model\", help=\"path to output model\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "#Run this part for the first time and save the pre-processed data. So you do not have to process it every time\n",
    "train_data = r\"D:\\TrafficSign\\Collaborative Learning\\Collaborative Learning\\Combined training only.csv\"\n",
    "test_data_China = r\"D:\\TrafficSign\\Collaborative Learning\\Collaborative Learning\\TSRD Test separated correct.csv\"\n",
    "test_data_German = r\"D:\\TrafficSign\\Collaborative Learning\\Collaborative Learning\\GTSRB test separated.csv\"\n",
    "(train_X, train_Y) = load_training_data(train_data)\n",
    "np.save('Collaborative_trainX',train_X)\n",
    "np.save('Collaborative_trainY',train_Y)\n",
    "(test_X_German, test_Y_German) = load_testing_data(test_data_German)\n",
    "np.save('Collaborative_GTSD_testX',test_X_German)\n",
    "np.save('Collaborative_GTSD_testY',test_Y_German)\n",
    "(test_X_China, test_Y_China) = load_testing_data(test_data_China)\n",
    "np.save('Collaborative_TSRD_testX',test_X_China)\n",
    "np.save('Collaborative_TSRD_testY',test_Y_China)\n",
    "\n",
    "\n",
    "# train_X = np.load('Collaborative_trainX.npy', allow_pickle=True)\n",
    "# train_Y = np.load('Collaborative_trainY.npy', allow_pickle=True)\n",
    "# test_X_China = np.load('Collaborative_TSRD_testX.npy', allow_pickle=True)\n",
    "# test_Y_China = np.load('Collaborative_TSRD_testY.npy', allow_pickle=True)\n",
    "# test_X_German = np.load('Collaborative_GTSD_testX.npy', allow_pickle=True)\n",
    "# test_Y_German = np.load('Collaborative_GTSD_testY.npy', allow_pickle=True)\n",
    "print(\"UPDATE: Normalizing data\")\n",
    "trainX = train_X.astype(\"float32\") / 255.0\n",
    "testX_China = test_X_China.astype(\"float32\") / 255.0\n",
    "testX_German = test_X_German.astype(\"float32\") / 255.0\n",
    "print(\"UPDATE: One-Hot Encoding data\")\n",
    "num_labels = len(np.unique(train_Y))\n",
    "trainY = to_categorical(train_Y, num_labels)\n",
    "testY_China = to_categorical(test_Y_China, num_labels)\n",
    "testY_German = to_categorical(test_Y_German, num_labels)\n",
    "\n",
    "class_totals = trainY.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals\n",
    "\n",
    "data_aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.15,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "vertical_flip=False)\n",
    "\n",
    "model = RoadSignClassifier.createCNN(width=32, height=32, depth=3, classes=num_labels)\n",
    "optimizer = Adam(learning_rate=learning_rate, decay=learning_rate / (epochs))\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = AdditionalValidationSets([(testX_China, testY_China, 'val2')])\n",
    "\n",
    "fit = model.fit(\n",
    "    data_aug.flow(trainX, trainY, batch_size=batch_size), \n",
    "    epochs=epochs,\n",
    "    validation_data=(testX_German, testY_German),\n",
    "    # class_weight=class_weight,\n",
    "    callbacks=[history],\n",
    "    verbose=1)\n",
    "\n",
    "# model.save(args[\"model\"])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc_German = history.history['val_accuracy']\n",
    "val_acc_China = history.history['val2_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss_German = history.history['val_loss']\n",
    "val_loss_China = history.history['val2_loss']\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc_German, label='German Validation Accuracy')\n",
    "plt.plot(val_acc_China, label='China Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss_German, label='German Validation Loss')\n",
    "plt.plot(val_loss_China, label='China Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "# plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "np.save('Collaborative_German_and_China_mc6', [acc,loss,val_acc_German,val_loss_German,val_acc_China,val_loss_China])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Collaborative_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
