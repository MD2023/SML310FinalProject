{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTIDNrD4iYrF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "data_path = r\"D:\\TrafficSign\\TSRD\"\n",
    "def load_data(dataset):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        image = os.path.join(data_path, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        image_rs = cv2.resize(image, (32, 32), 3)\n",
    "        R, G, B = cv2.split(image_rs)\n",
    "        img_r = cv2.equalizeHist(R)\n",
    "        img_g = cv2.equalizeHist(G)\n",
    "        img_b = cv2.equalizeHist(B)\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(img_class)\n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    \n",
    "    return (X, y)\n",
    "        \n",
    "epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "train_X = np.load('TSRD_trainX.npy', allow_pickle=True)\n",
    "test_X = np.load('TSRD_testX.npy', allow_pickle=True)\n",
    "train_Y = np.load('TSRD_trainY.npy', allow_pickle=True)\n",
    "test_Y = np.load('TSRD_testY.npy', allow_pickle=True)\n",
    "print(\"UPDATE: Normalizing data\")\n",
    "trainX = train_X.astype(\"float32\") / 255.0\n",
    "testX = test_X.astype(\"float32\") / 255.0\n",
    "print(\"UPDATE: One-Hot Encoding data\")\n",
    "num_labels = len(np.unique(train_Y))\n",
    "trainY = to_categorical(train_Y, num_labels)\n",
    "testY = to_categorical(test_Y, num_labels)\n",
    "\n",
    "class_totals = trainY.sum(axis=0)\n",
    "class_weights = class_totals.max() / class_totals\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "data_aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.15,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "vertical_flip=False)\n",
    "\n",
    "# import pre-trained model\n",
    "base_model = tf.keras.models.load_model('output/trafficsignnet.model')\n",
    "optimizer = Adam(learning_rate=learning_rate, decay=learning_rate / (epochs))\n",
    "\n",
    "model = Sequential()\n",
    "for layer in base_model.layers[:-1]: # go through the base model (the number -2 determines the number of trainable layers)\n",
    "    layer.trainable = False # freeze the layer\n",
    "    model.add(layer)\n",
    "model.add(Dense(58, activation='softmax'))\n",
    "\n",
    "initial_layer1_weights_values = model.layers[0].get_weights()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "fit = model.fit(\n",
    "    data_aug.flow(trainX, trainY, batch_size=batch_size), \n",
    "    epochs=epochs,\n",
    "    validation_data=(testX, testY),\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1)\n",
    "\n",
    "final_layer1_weights_values = model.layers[0].get_weights()\n",
    "#check if layer[0] has changed during the training, the other layers may be checked in a similar way\n",
    "np.testing.assert_allclose(         \n",
    "    initial_layer1_weights_values[0], final_layer1_weights_values[0]\n",
    ")\n",
    "np.testing.assert_allclose(\n",
    "    initial_layer1_weights_values[1], final_layer1_weights_values[1]\n",
    ")\n",
    "\n",
    "\n",
    "acc = fit.history['accuracy']\n",
    "val_acc = fit.history['val_accuracy']\n",
    "\n",
    "loss = fit.history['loss']\n",
    "val_loss = fit.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "# plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Transder_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
