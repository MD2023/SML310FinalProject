{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OzcjcJwp3L_I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded: 0\n",
      "loaded: 500\n",
      "loaded: 1000\n",
      "loaded: 1500\n",
      "loaded: 2000\n",
      "loaded: 2500\n",
      "loaded: 3000\n",
      "loaded: 3500\n",
      "loaded: 4000\n",
      "loaded: 4500\n",
      "loaded: 5000\n",
      "loaded: 5500\n",
      "loaded: 6000\n",
      "loaded: 6500\n",
      "loaded: 7000\n",
      "loaded: 7500\n",
      "loaded: 8000\n",
      "loaded: 8500\n",
      "loaded: 9000\n",
      "loaded: 9500\n",
      "loaded: 10000\n",
      "loaded: 10500\n",
      "loaded: 11000\n",
      "loaded: 11500\n",
      "loaded: 12000\n",
      "loaded: 12500\n",
      "loaded: 13000\n",
      "loaded: 13500\n",
      "loaded: 14000\n",
      "loaded: 14500\n",
      "loaded: 15000\n",
      "loaded: 15500\n",
      "loaded: 16000\n",
      "loaded: 16500\n",
      "loaded: 17000\n",
      "loaded: 17500\n",
      "loaded: 18000\n",
      "loaded: 18500\n",
      "loaded: 19000\n",
      "loaded: 19500\n",
      "loaded: 20000\n",
      "loaded: 20500\n",
      "loaded: 21000\n",
      "loaded: 21500\n",
      "loaded: 22000\n",
      "loaded: 22500\n",
      "loaded: 23000\n",
      "loaded: 23500\n",
      "loaded: 24000\n",
      "loaded: 24500\n",
      "loaded: 25000\n",
      "loaded: 25500\n",
      "loaded: 26000\n",
      "loaded: 26500\n",
      "loaded: 27000\n",
      "loaded: 27500\n",
      "loaded: 28000\n",
      "loaded: 28500\n",
      "loaded: 29000\n",
      "loaded: 29500\n",
      "loaded: 30000\n",
      "loaded: 30500\n",
      "loaded: 31000\n",
      "loaded: 31500\n",
      "loaded: 32000\n",
      "loaded: 32500\n",
      "loaded: 33000\n",
      "loaded: 33500\n",
      "loaded: 34000\n",
      "loaded: 34500\n",
      "loaded: 35000\n",
      "loaded: 35500\n",
      "loaded: 36000\n",
      "loaded: 36500\n",
      "loaded: 37000\n",
      "loaded: 37500\n",
      "loaded: 38000\n",
      "loaded: 38500\n",
      "loaded: 39000\n",
      "loaded: 0\n",
      "loaded: 500\n",
      "loaded: 1000\n",
      "loaded: 1500\n",
      "loaded: 2000\n",
      "loaded: 2500\n",
      "loaded: 3000\n",
      "loaded: 3500\n",
      "loaded: 4000\n",
      "loaded: 4500\n",
      "loaded: 5000\n",
      "loaded: 5500\n",
      "loaded: 6000\n",
      "loaded: 6500\n",
      "loaded: 7000\n",
      "loaded: 7500\n",
      "loaded: 8000\n",
      "loaded: 8500\n",
      "loaded: 9000\n",
      "loaded: 9500\n",
      "loaded: 10000\n",
      "loaded: 10500\n",
      "loaded: 11000\n",
      "loaded: 11500\n",
      "loaded: 12000\n",
      "loaded: 12500\n",
      "UPDATE: Normalizing data\n",
      "UPDATE: One-Hot Encoding data\n",
      "Epoch 1/20\n",
      "613/613 [==============================] - 75s 119ms/step - loss: 1.4521 - accuracy: 0.5717 - val_loss: 0.5761 - val_accuracy: 0.8165\n",
      "Epoch 2/20\n",
      "613/613 [==============================] - 75s 123ms/step - loss: 0.6572 - accuracy: 0.7882 - val_loss: 0.4024 - val_accuracy: 0.8833\n",
      "Epoch 3/20\n",
      "613/613 [==============================] - 66s 108ms/step - loss: 0.4404 - accuracy: 0.8581 - val_loss: 0.4259 - val_accuracy: 0.8819\n",
      "Epoch 4/20\n",
      "613/613 [==============================] - 86s 140ms/step - loss: 0.3274 - accuracy: 0.8944 - val_loss: 0.3507 - val_accuracy: 0.8997\n",
      "Epoch 5/20\n",
      "613/613 [==============================] - 94s 154ms/step - loss: 0.2774 - accuracy: 0.9103 - val_loss: 0.2363 - val_accuracy: 0.9304\n",
      "Epoch 6/20\n",
      "613/613 [==============================] - 56s 92ms/step - loss: 0.2292 - accuracy: 0.9247 - val_loss: 0.2909 - val_accuracy: 0.9200\n",
      "Epoch 7/20\n",
      "613/613 [==============================] - 57s 92ms/step - loss: 0.2102 - accuracy: 0.9329 - val_loss: 0.2278 - val_accuracy: 0.9335\n",
      "Epoch 8/20\n",
      "613/613 [==============================] - 56s 91ms/step - loss: 0.1798 - accuracy: 0.9423 - val_loss: 0.2196 - val_accuracy: 0.9400\n",
      "Epoch 9/20\n",
      "613/613 [==============================] - 58s 95ms/step - loss: 0.1726 - accuracy: 0.9439 - val_loss: 0.1918 - val_accuracy: 0.9438\n",
      "Epoch 10/20\n",
      "613/613 [==============================] - 54s 88ms/step - loss: 0.1526 - accuracy: 0.9503 - val_loss: 0.2385 - val_accuracy: 0.9419\n",
      "Epoch 11/20\n",
      "613/613 [==============================] - 58s 95ms/step - loss: 0.1451 - accuracy: 0.9534 - val_loss: 0.1900 - val_accuracy: 0.9459\n",
      "Epoch 12/20\n",
      "613/613 [==============================] - 56s 91ms/step - loss: 0.1345 - accuracy: 0.9573 - val_loss: 0.1753 - val_accuracy: 0.9503\n",
      "Epoch 13/20\n",
      "613/613 [==============================] - 55s 89ms/step - loss: 0.1247 - accuracy: 0.9606 - val_loss: 0.1615 - val_accuracy: 0.9566\n",
      "Epoch 14/20\n",
      "613/613 [==============================] - 56s 91ms/step - loss: 0.1222 - accuracy: 0.9612 - val_loss: 0.1573 - val_accuracy: 0.9568\n",
      "Epoch 15/20\n",
      "613/613 [==============================] - 55s 90ms/step - loss: 0.1153 - accuracy: 0.9633 - val_loss: 0.1626 - val_accuracy: 0.9554\n",
      "Epoch 16/20\n",
      "613/613 [==============================] - 56s 92ms/step - loss: 0.1089 - accuracy: 0.9662 - val_loss: 0.2305 - val_accuracy: 0.9318\n",
      "Epoch 17/20\n",
      "613/613 [==============================] - 55s 90ms/step - loss: 0.0984 - accuracy: 0.9679 - val_loss: 0.1871 - val_accuracy: 0.9477\n",
      "Epoch 18/20\n",
      "613/613 [==============================] - 56s 92ms/step - loss: 0.0971 - accuracy: 0.9686 - val_loss: 0.2099 - val_accuracy: 0.9456\n",
      "Epoch 19/20\n",
      "613/613 [==============================] - 55s 89ms/step - loss: 0.0936 - accuracy: 0.9695 - val_loss: 0.1686 - val_accuracy: 0.9544\n",
      "Epoch 20/20\n",
      "613/613 [==============================] - 56s 91ms/step - loss: 0.0886 - accuracy: 0.9717 - val_loss: 0.1538 - val_accuracy: 0.9584\n",
      "INFO:tensorflow:Assets written to: output/trafficsignnet.model/assets\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import argparse\n",
    "\n",
    "# create neural network\n",
    "class RoadSignClassifier:\n",
    "    def createCNN(width, height, depth, classes):\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        model.add(Conv2D(8, (5, 5), input_shape=inputShape, activation=\"relu\"))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(512, activation=\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        return model\n",
    "\n",
    "\n",
    "# read images and resize\n",
    "data_path = r\"/Users/hdai/Documents/Junior Year/SML310/Final Project/GTSRB\"\n",
    "def load_data(dataset):\n",
    "    images = []\n",
    "    classes = []\n",
    "    rows = pd.read_csv(dataset)\n",
    "    rows = rows.sample(frac=1).reset_index(drop=True)\n",
    "    for i, row in rows.iterrows():\n",
    "        img_class = row[\"ClassId\"]\n",
    "        img_path = row[\"Path\"]\n",
    "        image = os.path.join(data_path, img_path)\n",
    "        image = cv2.imread(image)\n",
    "        image_rs = cv2.resize(image, (32, 32), 3)\n",
    "        R, G, B = cv2.split(image_rs)\n",
    "        img_r = cv2.equalizeHist(R)\n",
    "        img_g = cv2.equalizeHist(G)\n",
    "        img_b = cv2.equalizeHist(B)\n",
    "        new_image = cv2.merge((img_r, img_g, img_b))\n",
    "        if i % 500 == 0:\n",
    "            print(f\"loaded: {i}\")\n",
    "        images.append(new_image)\n",
    "        classes.append(img_class)\n",
    "    X = np.array(images)\n",
    "    y = np.array(classes)\n",
    "    \n",
    "    return (X, y)\n",
    "\n",
    "#ap = argparse.ArgumentParser()\n",
    "#ap.add_argument(\"-m\", \"--model\", default=\"output/trafficsignnet.model\", help=\"path to output model\")\n",
    "#args = vars(ap.parse_args())\n",
    "\n",
    "# set conditions\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "#Run this part for the first time and save the pre-processed data. So you do not have to process it every time\n",
    "train_data = r\"/Users/hdai/Documents/Junior Year/SML310/Final Project/GTSRB/Train.csv\"\n",
    "test_data = r\"/Users/hdai/Documents/Junior Year/SML310/Final Project/GTSRB/Test.csv\"\n",
    "(train_X, train_Y) = load_data(train_data)\n",
    "(test_X, test_Y) = load_data(test_data)\n",
    "np.save('GTSD_trainX',train_X)\n",
    "np.save('GTSD_trainY',train_Y)\n",
    "np.save('GTSD_testX',test_X)\n",
    "np.save('GTSD_testY',test_Y)\n",
    "\n",
    "# load data\n",
    "train_X = np.load('GTSD_trainX.npy', allow_pickle=True)\n",
    "test_X = np.load('GTSD_testX.npy', allow_pickle=True)\n",
    "train_Y = np.load('GTSD_trainY.npy', allow_pickle=True)\n",
    "test_Y = np.load('GTSD_testY.npy', allow_pickle=True)\n",
    "print(\"UPDATE: Normalizing data\")\n",
    "trainX = train_X.astype(\"float32\") / 255.0\n",
    "testX = test_X.astype(\"float32\") / 255.0\n",
    "print(\"UPDATE: One-Hot Encoding data\")\n",
    "num_labels = len(np.unique(train_Y))\n",
    "trainY = to_categorical(train_Y, num_labels)\n",
    "testY = to_categorical(test_Y, num_labels)\n",
    "\n",
    "class_totals = trainY.sum(axis=0)\n",
    "class_weight = class_totals.max() / class_totals\n",
    "\n",
    "# data augmentation\n",
    "data_aug = ImageDataGenerator(\n",
    "rotation_range=10,\n",
    "zoom_range=0.15,\n",
    "width_shift_range=0.1,\n",
    "height_shift_range=0.1,\n",
    "shear_range=0.15,\n",
    "horizontal_flip=False,\n",
    "vertical_flip=False)\n",
    "\n",
    "# train model\n",
    "model = RoadSignClassifier.createCNN(width=32, height=32, depth=3, classes=43)\n",
    "optimizer = Adam(learning_rate=learning_rate, decay=learning_rate / (epochs))\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "fit = model.fit(\n",
    "    data_aug.flow(trainX, trainY, batch_size=batch_size), \n",
    "    epochs=epochs,\n",
    "    validation_data=(testX, testY),\n",
    "    # class_weight=class_weight,\n",
    "    verbose=1)\n",
    "model.save(\"output/trafficsignnet.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x7fbd9d3aa280>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.models.load_model('output/trafficsignnet.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Individual_Learning_German.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
